\documentclass[12pt, a4paper]{article}
\usepackage{geometry}
\usepackage{titlesec}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\usepackage{graphicx}
\usepackage{float}

\geometry{margin=1in}

\title{\textbf{Memory Management Simulator \\ Design Document}}
\author{}
\date{\today}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Introduction}
This document describes the design and implementation of the Memory Management Simulator. The simulator models various components of a computer's memory hierarchy, including physical memory allocation, a buddy system, a multi-level cache, and virtual memory with paging.

\section{Memory Layout and Assumptions}

\subsection{Physical Memory Model}
The physical memory is simulated as a contiguous block of bytes, implemented using a `std::vector<char>`. This vector represents the raw storage available to the system.

\subsection{Block Structure}
Memory is managed utilizing `BlockHeader` structures embedded within the physical memory itself. Each allocated or free block is preceded by a header containing:
\begin{itemize}
    \item \textbf{Size}: The size of the payload (excluding the header).
    \item \textbf{Status}: A boolean flag indicating if the block is free or allocated.
    \item \textbf{Pointers}: `next` and `prev` pointers to maintain a doubly linked list of blocks.
    \item \textbf{Metadata}: An ID for allocation tracking and padding information for alignment.
\end{itemize}

\subsection{Assumptions}
\begin{itemize}
    \item Memory is byte-addressable.
    \item All allocations are aligned to 8-byte boundaries.
    \item The system simulates a single process environment.
    \item Pointers returned to the user are simulated addresses (offsets into the memory vector).
\end{itemize}

\section{Allocation Strategy Implementations}

The simulator supports three standard dynamic memory allocation strategies, managed by the `MemoryManager` class.

\subsection{First Fit}
The allocator traverses the linked list of memory blocks from the beginning (head) and selects the \textbf{first} free block that is large enough to satisfy the request. This approach minimizes search time but may lead to fragmentation at the beginning of memory.

\subsection{Best Fit}
The allocator traverses the entire list to find the free block that is closest in size to the requested amount (smallest difference). This minimizes internal fragmentation for the specific request but is slower due to the full search.

\subsection{Worst Fit}
The allocator traverses the entire list to find the largest available free block. The rationale is that the remaining portion of the split block will be large enough to be useful for future requests.

\subsection{Coalescing}
Upon deallocation (`free`), the system checks the immediate previous and next neighbors. If they are free, they are merged with the current block to form a larger contiguous free block, reducing external fragmentation.

\section{Buddy System Design}

The Buddy Allocator (`BuddyAllocator`) is implemented as a separate module compliant with power-of-two memory management principles.

\subsection{Design}
\begin{itemize}
    \item \textbf{Free Lists}: An array of linked lists (`free_lists`), where index $i$ stores free blocks of size $2^i$.
    \item \textbf{Splitting}: When a request for size $2^k$ arrives, if no block of order $k$ is available, the system recursively looks for a block of order $k+1$, splits it into two "buddies" of order $k$, adds one to the free list, and returns the other.
    \item \textbf{Coalescing}: When a block of order $k$ is freed, its buddy's address is calculated using the XOR operation: $\text{BuddyAddr} = \text{BlockAddr} \oplus \text{Size}$. If the buddy is also free, they are merged into a block of order $k+1$, and the process repeats recursively up to the maximum order.
\end{itemize}

\section{Cache Hierarchy and Replacement Policy}

\subsection{Cache Hierarchy}
The system implements a configurable three-level cache hierarchy:
\begin{itemize}
    \item \textbf{L1 Cache}: Small, fast, direct-mapped (default: 64B, 1-way).
    \item \textbf{L2 Cache}: Medium, set-associative (default: 256B, 2-way).
    \item \textbf{L3 Cache}: Large, highly associative (default: 1KB, 8-way).
\end{itemize}
The hierarchy supports multiple replacement policies (FIFO, LRU, LFU) configurable via the CLI. Accesses propagate from L1 to L3; a hit at any level returns the data, while a miss at the last level triggers a memory access (or virtual memory translation).

\subsection{Replacement Policy}
The system supports multiple replacement policies, configurable at runtime:
\begin{itemize}
    \item \textbf{FIFO (First-In-First-Out)}: Evicts the block that was installed earliest (Default).
    \item \textbf{LRU (Least Recently Used)}: Evicts the block that has not been accessed for the longest time.
    \item \textbf{LFU (Least Frequently Used)}: Evicts the block with the fewest accesses (ties broken by LRU).
\end{itemize}

Each cache set maintains necessary metadata (timestamps, access counts) to implement these policies efficiently.

\section{Virtual Memory Model}

The `VirtualMemoryManager` simulates paging and address translation.

\subsection{Page Tables}
\begin{itemize}
    \item \textbf{Virtual Address Space}: Divided into fixed-size pages (e.g., 64 bytes).
    \item \textbf{Page Table}: A vector of `PageTableEntry` structures, indexed by Page Number. Each entry stores the Frame Number, Valid bit, Dirty bit, and Access Time.
    \item \textbf{Frame Table}: A vector tracking the allocation status of physical memory frames.
\end{itemize}

\subsection{Address Translation Flow}
When the user requests an access (Read/Write) to a Virtual Address:
1.  **Bit Slicing**: The Virtual Address is split into \textit{Page Number} and \textit{Offset}.
2.  **TLB/Page Table Lookup**: The system checks the Page Table.
3.  **Page Fault Handling**:
    \begin{itemize}
        \item If the `Valid` bit is false, a \textbf{Page Fault} occurs.
        \item The system finds a free physical frame. If none are available, a page is evicted based on the replacement policy.
        \item The mapping is updated, and the `Valid` bit is set.
    \end{itemize}
4.  **Physical Address Generation**: $\text{Physical Address} = (\text{Frame Number} \times \text{Page Size}) + \text{Offset}$.
5.  **Memory Access**: The generated Physical Address is passed to the Cache System.

\subsection{Page Replacement}
The system supports **FIFO** (default) and **LRU** policies for page eviction.
\begin{itemize}
    \item \textbf{FIFO}: Evicts the page that was loaded earliest.
    \item \textbf{LRU}: Evicts the page with the oldest `last_access_time`.
\end{itemize}

\section{Integration and Address Flow}

The components interact in a sequential pipeline:
\begin{verbatim}
[CLI Command] -> [Virtual Memory (Translate)] -> [Physical Address] 
              -> [Cache Hierarchy (Access)] -> [Stats Update]
\end{verbatim}

\noindent When `malloc` is called, it operates on the logical heap space. When `read/write` commands are issued, they trigger the full simulation pipeline, updating Page Fault, Cache Hit/Miss, and Memory Access statistics.

\section{Limitations and Simplifications}
\begin{itemize}
    \item \textbf{Data Storage}: The simulator accurately tracks metadata (validity, tags, mappings) but does not fully persist user data values through the Cache/VM hierarchy layers (e.g., values written to L1 are not mechanically flushed to RAM `vector` on eviction in the simulation code, though the `write` command updates stats).
    \item \textbf{Single Threaded}: The environment assumes a single-threaded access pattern; race conditions are not modeled.
    \item \textbf{Disk Simulation}: Page faults equate to "loading," but no actual disk I/O latency or file storage is implemented.
    \item \textbf{Address Space}: The virtual address space size is fixed at initialization for simplicity.
\end{itemize}

\section{Usage Guide}
The simulator is an interactive command-line interface (CLI) application.

\subsection{Basic Commands}
\begin{itemize}
    \item \texttt{init <size>}: Initialize physical memory with \texttt{size} bytes. Default allocation strategy is First Fit. cache is also initialized with default settings.
    \item \texttt{malloc <size>}: Allocate \texttt{size} bytes. Returns a memory address (or ID).
    \item \texttt{free <id|addr>}: Free memory block by ID (integer) or Address (offset).
    \item \texttt{dump}: Display a visual map of the physical memory state.
    \item \texttt{stats}: Show detailed statistics for Memory (fragmentation), Cache (hits/misses), and Virtual Memory (faults/hits).
    \item \texttt{set <strategy>}: Change allocation strategy. Options: \texttt{first fit}, \texttt{best fit}, \texttt{worst fit}, \texttt{buddy}.
    \item \texttt{set cache policy <fifo|lru|lfu>}: Change cache replacement policy.
    \item \texttt{set vm policy <fifo|lru|clock>}: Change VM page replacement policy.
    \item \texttt{set vm latency <ms>}: Set simulated disk I/O latency in milliseconds.
\end{itemize}

\subsection{Advanced Features}
The simulator supports advanced features such as:
\begin{itemize}
    \item \textbf{Clock Algorithm}: An approximation of LRU using reference bits.
    \item \textbf{Disk Latency Simulation}: Configurable delay during page faults.
\end{itemize}
\begin{itemize}
    \item \texttt{enable\_vm <page\_size>}: Enable Virtual Memory simulation with a specified page size. This enables address translation for subsequent access commands.
    \item \texttt{read <addr>}: Simulate a read access to address \texttt{addr}. Triggers VM translation and Cache access.
    \item \texttt{write <addr> <val>}: Simulate a write access to address \texttt{addr}. Triggers VM translation and Cache access.
\end{itemize}

\section{Example Verification Case}

This section demonstrates the system's correctness using a sample test trace.

\subsection{Test Input}
\begin{verbatim}
init 256
enable_vm 64
read 0
read 0
read 64
stats
\end{verbatim}

\subsection{Expected Behavior \& Explanation}
\begin{enumerate}
    \item \textbf{Initialization}: Physical memory is set to 256 bytes (4 frames of 64 bytes).
    \item \textbf{Enable VM}: Virtual Memory enabled with 64-byte pages.
    \item \textbf{Read 0 (1st time)}:
        \begin{itemize}
            \item \textit{VM}: Page 0 is not in memory. \textbf{Page Fault}. Frame 0 allocated. Virtual 0 mapped to Physical 0.
            \item \textit{Cache}: Physical 0 accessed. \textbf{Miss} (Cold). L1 loaded.
        \end{itemize}
    \item \textbf{Read 0 (2nd time)}:
        \begin{itemize}
            \item \textit{VM}: Page 0 is valid. \textbf{Page Hit}.
            \item \textit{Cache}: Physical 0 tag matches in L1. \textbf{Cache Hit}.
        \end{itemize}
    \item \textbf{Read 64 (1st time)}:
        \begin{itemize}
            \item \textit{VM}: Page 1 is not in memory. \textbf{Page Fault}. Frame 1 allocated. Virtual 64 mapped to Physical 64.
            \item \textit{Cache}: Physical 64 accessed (different index/tag). \textbf{Miss}.
        \end{itemize}
\end{enumerate}

\subsection{Actual Output}
The simulator produces the following statistics, confirming the logic:

\begin{verbatim}
=== Cache Statistics ===
L1 Cache Stats:
  Hits: 1
  Misses: 2
  Hit Rate: 33.33%

=== Virtual Memory Statistics ===
  Page Faults: 2
  Page Hits:   1
  Hit Rate:    33.33%
\end{verbatim}

\noindent \textbf{Analysis}:
\begin{itemize}
    \item \textbf{Page Faults = 2}: One for Page 0 (first access), one for Page 1. Correct.
    \item \textbf{Page Hits = 1}: Second access to Page 0. Correct.
    \item \textbf{Cache Hits = 1}: Second access to address 0. Correct.
    \item \textbf{Cache Misses = 2}: First access to 0 and first access to 64. Correct.
\end{itemize}

\section{Comprehensive Feature Test}

This scenario demonstrates the integration of all major features: Buddy Allocator, Virtual Memory, and Cache.

\subsection{Test Input}
\begin{verbatim}
init 1024
set allocator buddy
malloc 128
enable_vm 64
read 0
read 0
read 64
stats
\end{verbatim}

\subsection{Execution Trace}
\begin{enumerate}
    \item \textbf{Initialization}: Memory 1024 bytes.
    \item \textbf{Buddy Allocation}: 
        \begin{itemize}
            \item User requests 128 bytes.
            \item Buddy Allocator rounds 128 + Header(32) = 160 -> Order 8 (256 bytes).
            \item Splits Order 10 (1024) -> Order 9 (512) -> Order 8 (256).
            \item Returns address.
        \end{itemize}
    \item \textbf{Virtual Memory Enable}: Page Size 64. Memory treated as 16 Phys Frames.
    \item \textbf{Access Flow}:
        \begin{itemize}
            \item \texttt{read 0}: **Page Fault**. Frame 0 allocated. L1 **Miss**. Loaded.
            \item \texttt{read 0}: **Page Hit**. L1 **Hit**.
            \item \texttt{read 64}: **Page Fault**. Frame 1 allocated. L1 **Miss**.
        \end{itemize}
\end{enumerate}

\subsection{Results Analysis}
The output confirms correct operation of all layers:
\begin{itemize}
    \item \textbf{Allocation}: "Buddy Alloc: Order 8 (256 bytes)" confirm buddy logic is active.
    \item \textbf{VM}: "Page Faults: 2" confirms paging is intercepting addresses.
    \item \textbf{Cache}: "Misses: 2" confirms cache is receiving physical addresses.
\end{itemize}

\end{document}
